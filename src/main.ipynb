{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150c0711",
   "metadata": {},
   "source": [
    "# PCA\n",
    "-> [2d_example.ipynb](2d_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce3e1f",
   "metadata": {},
   "source": [
    "$n$個の$d$次元のデータ $X$ を、$d'$次元に圧縮するための線形変換行列$W$を求める。\n",
    "\n",
    "$$\n",
    "X\n",
    "= \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W\n",
    "= \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1,d'} \\\\\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2,d'} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{d,1} & w_{d,2} & \\cdots & w_{d,d'}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{d \\times d'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47fae0",
   "metadata": {},
   "source": [
    "変換後のデータ $Y$ は次のように表される。\n",
    "$$\n",
    "Y = X W \\in \\mathbb{R}^{n \\times d'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebea393",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "PCAは、圧縮後の各次元の分散を最大化することを考える。  \n",
    "次元 $j$ の分散は、定義より以下で表せる\n",
    "$$\n",
    "\\begin{align*}\n",
    "s_j^2 \n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}(y_{ij}-\\bar{y_j})^2\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "    X_i W_j - \\frac{1}{n}\\sum_{k=1}^{n}(X_k W_j)\n",
    "\\right)^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "    X_i W_j - \\frac{1}{n}\\sum_{k=1}^{n}(X_k) W_j\n",
    "\\right)^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "    \\left(X_i - \\frac{1}{n}\\sum_{k=1}^{n}X_k\\right) W_j\n",
    "\\right)^2 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49758ded",
   "metadata": {},
   "source": [
    "ここで、中心化されたデータを $X^c$ とすると、\n",
    "$$\n",
    "\\begin{align*}\n",
    "s_j^2 \n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}(X_i^c W_j)^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}(X_i^cW_j)^\\top(X_i^cW_j) \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}W_j^\\top(X_i^c)^\\top(X_i^c)W_j \\\\\n",
    "&= W_j^\\top\\left(\\frac{1}{n}\\sum_{i=1}^{n}(X_i^c)^\\top(X_i^c)\\right)W_j\\\\\n",
    "&= W_j^\\top\\left(\\frac{1}{n}X^{c\\top}X^c\\right)W_j \\\\\n",
    "&= W_j^\\top S W_j \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50cd778",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "ここで、$S$ はデータの共分散行列である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d77aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "PCAでは、$s_j^2$ を最大化する $W_j$ を求める。\n",
    "なお、ここで$W_j$を大きくすれば$s_j^2$も大きくなるため、$W_j$は単位ベクトル、つまり $$ \\|W_j\\|^2 = W_j^\\top W_j = 1 $$ とする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71586f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "結果、解きたい問題は\n",
    "$$\n",
    "\\underset{W_j}{\\text{argmax}}\\quad W_j^\\top S W_j \\quad \\text{subject to}\\quad  W_j^\\top W_j = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cd8f0",
   "metadata": {},
   "source": [
    "これをラグランジュの未定乗数法を用いて解くことを考える。ラグランジュ関数は以下のようになる。\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(W_j, \\lambda) &= W_j^\\top S W_j - \\lambda (W_j^\\top W_j - 1) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c75662",
   "metadata": {},
   "source": [
    "これを$W_j$で微分すれば、\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_j} &= 2 S W_j - 2 \\lambda W_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b2328",
   "metadata": {},
   "source": [
    "したがって、以下を満たすときに極値をとる。\n",
    "$$\n",
    "S W_j = \\lambda W_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1b50c",
   "metadata": {},
   "source": [
    "したがって、$W_j$は$S$の固有ベクトルであり、$\\lambda$は対応する固有値である。  \n",
    "また、$s_j^2$は、以下のように書き換えられる。\n",
    "$$\n",
    "\\begin{align*}\n",
    "s_j^2 &= W_j^\\top S W_j \\\\\n",
    "&= W_j^\\top \\lambda W_j \\\\\n",
    "&= \\lambda W_j^\\top W_j \\\\\n",
    "&= \\lambda\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588974b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "結果、PCAで$d'$次元の次元圧縮を行う場合は、固有値が大きい$d'$個の$S$の固有ベクトルを並べれば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5325cd",
   "metadata": {},
   "source": [
    "## (参考) 固有ベクトルは固有値分解で求められる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df70b80",
   "metadata": {},
   "source": [
    "\n",
    "行列 $A \\in \\mathbb{R}^{d\\times d}$ の固有値分解は、直交行列$V$を用いて以下のように表される。\n",
    "$$\n",
    "A = V \\Lambda V^\\top\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ecb173",
   "metadata": {},
   "source": [
    "ここで、\n",
    "$$\n",
    "\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_d)\n",
    "$$\n",
    "$$\n",
    "V = \\begin{bmatrix}\n",
    "v_1 & v_2 & \\cdots & v_d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "と表すと、以下のように変形できる。\n",
    "$$\n",
    "\\begin{align*}\n",
    "A &= V \\Lambda V^\\top\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "v_1 & v_2 & \\cdots & v_d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 v_1 \\\\\n",
    "\\lambda_2 v_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_d v_d\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\sum_{i=1}^{d} \\lambda_i v_i v_i^\\top\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093684b",
   "metadata": {},
   "source": [
    "両辺に右から$v_j$を掛けると、\n",
    "$$\n",
    "\\begin{align*}\n",
    "A v_j &= \\sum_{i=1}^{d} \\lambda_i v_i v_i^\\top v_j \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "$V$の直交性より、$v_i^\\top v_j = 0$ ($i \\neq j$) であるため、\n",
    "$$\n",
    "\\begin{align*}\n",
    "A v_j &= \\lambda_j v_j \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31e160",
   "metadata": {},
   "source": [
    "したがって、$v_j$は$A$の固有ベクトルであり、$\\lambda_j$は対応する固有値である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b83eb0",
   "metadata": {},
   "source": [
    "## $X$の特異値分解 (SVD) で求める\n",
    "-> [10d_example.ipynb](10d_example.ipynb), [torch_svd.ipynb](torch_svd.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed781b8d",
   "metadata": {},
   "source": [
    "$S$ は $d\\times d$ の行列である。  \n",
    "以下が成り立つため、実装上は$X$の右特異ベクトルを求めれば良い。\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S &= \\frac{1}{n} X^{c\\top} X^c \\\\\n",
    "&= \\frac{1}{n} \\left(U\\Sigma V^{\\top}\\right)^\\top \\left(U\\Sigma V^{\\top}\\right) \\\\\n",
    "&= \\frac{1}{n} V\\Sigma U^{\\top} U\\Sigma V^{\\top} \\\\\n",
    "&= \\frac{1}{n} V\\Sigma^{2} V^{\\top} \\\\\n",
    "&= V \\left(\\frac{\\Sigma^{2}}{n} \\right) V^{\\top} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68543126",
   "metadata": {},
   "source": [
    "# サンプリング確率 + 重複なしサンプリング\n",
    "-> [weighted_pca.ipynb](weighted_pca.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e70aa6",
   "metadata": {},
   "source": [
    "### 中心化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955094e",
   "metadata": {},
   "source": [
    "通常のPCAでは、平均を引くことで中心化が行われる。\n",
    "$$\n",
    "X^c_i = X_i - \\frac{1}{n} \\sum_{k=1}^{n} X_k\n",
    "$$\n",
    "ここで、データの重複を考えて、$n'$ 種類のデータが観測されたとする。\n",
    "また、$Z_i$ は $c_i$ 回観測されたとする。\n",
    "結果、以下のように中心化すれば良い。\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^c_i \n",
    "&= X_i - \\frac{1}{n} \\sum_{k=1}^{n'} c_k Z_k\\\\\n",
    "&= X_i - \\sum_{k=1}^{n'} \\frac{c_k}{n} Z_k\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96901c1c",
   "metadata": {},
   "source": [
    "### 分散"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41c981",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "s_j^2 \n",
    "&= W_j^\\top\\left(\\frac{1}{n}\\sum_{i=1}^{n}(X_i^c)^\\top(X_i^c)\\right)W_j\\\\\n",
    "&= W_j^\\top\\left(\\sum_{i=1}^{n}\\frac{1}{n}(X_i^c)^\\top(X_i^c)\\right)W_j\\\\\n",
    "&= W_j^\\top\\left(\\sum_{i=k}^{n'}\\frac{c_k}{n}(Z_k^c)^\\top(Z_k^c)\\right)W_j\\\\\n",
    "&= W_j^\\top\\left(\\sum_{i=k}^{n'}\\left(\\sqrt{\\frac{c_k}{n}}Z_k^c\\right)^\\top\\left(\\sqrt{\\frac{c_k}{n}}Z_k^c\\right)\\right)W_j\\\\\n",
    "&= W_j^\\top S W_j \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbb676",
   "metadata": {},
   "source": [
    "通常のPCAと同様に考えれば、$\\sqrt{\\frac{c_k}{n}}Z_k^c$の右特異ベクトルを求めれば良いことがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34205565",
   "metadata": {},
   "source": [
    "## Randomized SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097e903",
   "metadata": {},
   "source": [
    "大きな行列をSVDするときに効率の良い確率的な手法。\n",
    "-> [randomized_svd.ipynb](randomized_svd.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
